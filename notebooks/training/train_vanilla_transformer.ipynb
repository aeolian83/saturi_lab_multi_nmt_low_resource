{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# set path for module import from user's directory\n",
    "sys.path.insert(0, os.getenv('HOME') + '/saturi_lab_multi_nmt_low_resource/src/training/')\n",
    "sys.path.insert(1, os.getenv('HOME') + '/saturi_lab_multi_nmt_low_resource/src/utils')\n",
    "sys.path.insert(2, os.getenv('HOME') + '/saturi_lab_multi_nmt_low_resource/src/models/baseline/')\n",
    "\n",
    "from dataset_util import CustomDatasetforTranslation\n",
    "import utils\n",
    "from vanilla_transformer import Transformer, generate_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = os.getenv('HOME') + '/saturi_lab_multi_nmt_low_resource'\n",
    "data_path = main_path + '/data/processed/translated_train_data.csv'\n",
    "df = pd.read_csv(data_path,)\n",
    "del df['Unnamed: 0']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer model files path\n",
    "src_tok_path = main_path + '/saved_models/tokenizer/old/spm_enc_spm16000.model'\n",
    "tgt_tok_path = main_path + '/saved_models/tokenizer/old/spm_dec_spm16000.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenzier models\n",
    "\n",
    "src_tokenizer = spm.SentencePieceProcessor()#.Load(src_tok_path)\n",
    "src_tokenizer.Load(src_tok_path)\n",
    "tgt_tokenizer = spm.SentencePieceProcessor()#.Load(tgt_tok_path)\n",
    "tgt_tokenizer.Load(tgt_tok_path)\n",
    "tgt_tokenizer.set_encode_extra_options(\"bos:eos\")\n",
    "\n",
    "print('source tokenizer vocab size :',src_tokenizer.vocab_size())\n",
    "print(src_tokenizer.EncodeAsPieces('Here is an example of source tokenization.'))\n",
    "print('target tokenizer vocab size :',tgt_tokenizer.vocab_size())\n",
    "print(tgt_tokenizer.EncodeAsPieces('이것은 토큰화 예시입니다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2,stratify=df['reg'],random_state=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cretate dataset for training; CustomDataSetforTranslation class created in src folder\n",
    "train_dataset = CustomDatasetforTranslation(train['eng'].to_numpy(),train['dial'].to_numpy(),train['reg'].to_numpy(), 32, src_tokenizer,tgt_tokenizer)\n",
    "valid_dataset = CustomDatasetforTranslation(test['eng'].to_numpy(), test['dial'].to_numpy(), test['reg'].to_numpy(), 32, src_tokenizer,tgt_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Scheduler,optimizer, and loss function\n",
    "learningrate = utils.LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learningrate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = criterion(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train function\n",
    "\n",
    "@tf.function(reduce_retracing=True)\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the model\n",
    "@tf.function\n",
    "def model_validate(src, tgt, model):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "    v_loss = loss_function(gold, predictions[:, :-1])\n",
    "    \n",
    "    return v_loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def train(transformer,train_dataset, valid_dataset,optimizer,EPOCHS):\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        t = tqdm(train_dataset)\n",
    "        total_loss = 0\n",
    "        t.set_description_str(f'EPOCH {epoch}')\n",
    "\n",
    "        for i, pairs in enumerate(t):\n",
    "            \n",
    "            src, tgt = pairs\n",
    "            max_len = len(max(src,key=len))\n",
    "            enc_train = tf.keras.preprocessing.sequence.pad_sequences(src, padding='post', maxlen=max_len)\n",
    "            dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt, padding='post', maxlen=max_len)\n",
    "\n",
    "            batch_loss = train_step(enc_train,\n",
    "                                    dec_train,\n",
    "                                    transformer,\n",
    "                                    optimizer)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "\n",
    "            t.set_postfix_str('Loss %.4f' %  (total_loss.numpy() / (i + 1)))\n",
    "            \n",
    "            \n",
    "        #validation\n",
    "        total_loss_val = 0\n",
    "        tv = tqdm(valid_dataset)\n",
    "        \n",
    "        for k, vpairs in enumerate(tv) :\n",
    "            src, tgt = vpairs\n",
    "            max_len = len(max(src,key=len))\n",
    "            enc_val = tf.keras.preprocessing.sequence.pad_sequences(src, padding='post', maxlen=max_len)\n",
    "            dec_val = tf.keras.preprocessing.sequence.pad_sequences(tgt, padding='post', maxlen=max_len)\n",
    "            val_loss = model_validate(enc_val,\n",
    "                                      dec_val,\n",
    "                                      transformer)\n",
    "            total_loss_val += val_loss\n",
    "            tv.set_postfix_str('val_Loss %.4f' % (total_loss_val.numpy() / (k + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model config\n",
    "import json\n",
    "config_path = main_path + '/src/utils/config.json'\n",
    "with open(config_path,'r') as f :\n",
    "    config = json.load(f)['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['src_vocab_size'] = src_tokenizer.vocab_size()\n",
    "config['tgt_vocab_size'] = tgt_tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model init\n",
    "transformer = Transformer(\n",
    "    n_layers=config['n_layers'],\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    d_ff=config['d_ff'],\n",
    "    src_vocab_size=config['src_vocab_size'],\n",
    "    tgt_vocab_size=config['tgt_vocab_size'],\n",
    "    pos_len=config['pos_len'],\n",
    "    dropout=config['dropout'],\n",
    "    shared=config['shared'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train(transformer,train_dataset, valid_dataset,optimizer,config['epochs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
