{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# set path for module import from user's directory\n",
    "sys.path.insert(0, os.getenv('HOME') + '/saturi_lab_multi_nmt_low_resource/src/training/')\n",
    "sys.path.insert(1, os.getenv('HOME') + '/saturi_lab_multi_nmt_low_resource/src/utils')\n",
    "sys.path.insert(2, os.getenv('HOME') + '/saturi_lab_multi_nmt_low_resource/src/models/baseline/')\n",
    "\n",
    "from dataset_util import CustomDatasetforTranslation\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>dial</th>\n",
       "      <th>reg</th>\n",
       "      <th>pair</th>\n",
       "      <th>eng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>그 부주를 하기에는 조금 그렇지 않아?</td>\n",
       "      <td>그 부주를 하기에는 조금 그렇지 안?</td>\n",
       "      <td>jj</td>\n",
       "      <td>(안?)/(않아?)</td>\n",
       "      <td>Isnt it a little bit like that carelessness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그리고 거기서 밥을 먹을지도 모르겠어.</td>\n",
       "      <td>그리고 거기서 밥을 먹을지도 모르겐.</td>\n",
       "      <td>jj</td>\n",
       "      <td>(모르겐.)/(모르겠어.)</td>\n",
       "      <td>And I dont even know if Ill eat rice there</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>왜냐하면은 밥이 안 나온다고 들었어.</td>\n",
       "      <td>왜냐하면은 밥이 안 나온다고 들언.</td>\n",
       "      <td>jj</td>\n",
       "      <td>(들언.)/(들었어.)</td>\n",
       "      <td>Because I heard that the rice isnt served</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>토요일 날에 토요일날에 잔치를 하고 일요일 날에 식을 올린대.</td>\n",
       "      <td>토요일 날에 토요일날에 잔치를 하고 일요일 날에 식을 올린댄.</td>\n",
       "      <td>jj</td>\n",
       "      <td>(올린댄.)/(올린대.)</td>\n",
       "      <td>They have a party on Saturday and a ceremony o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>그럼 어떻게 혼자 아 혼자래.</td>\n",
       "      <td>그럼 어떻게 혼자 아 혼자랜.</td>\n",
       "      <td>jj</td>\n",
       "      <td>(혼자랜.)/(혼자래.)</td>\n",
       "      <td>Then how are you alone Oh are you alone</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 text                                dial reg   \n",
       "0               그 부주를 하기에는 조금 그렇지 않아?                그 부주를 하기에는 조금 그렇지 안?  jj  \\\n",
       "1               그리고 거기서 밥을 먹을지도 모르겠어.                그리고 거기서 밥을 먹을지도 모르겐.  jj   \n",
       "2                왜냐하면은 밥이 안 나온다고 들었어.                 왜냐하면은 밥이 안 나온다고 들언.  jj   \n",
       "3  토요일 날에 토요일날에 잔치를 하고 일요일 날에 식을 올린대.  토요일 날에 토요일날에 잔치를 하고 일요일 날에 식을 올린댄.  jj   \n",
       "4                    그럼 어떻게 혼자 아 혼자래.                    그럼 어떻게 혼자 아 혼자랜.  jj   \n",
       "\n",
       "             pair                                                eng  \n",
       "0      (안?)/(않아?)        Isnt it a little bit like that carelessness  \n",
       "1  (모르겐.)/(모르겠어.)         And I dont even know if Ill eat rice there  \n",
       "2    (들언.)/(들었어.)          Because I heard that the rice isnt served  \n",
       "3   (올린댄.)/(올린대.)  They have a party on Saturday and a ceremony o...  \n",
       "4   (혼자랜.)/(혼자래.)            Then how are you alone Oh are you alone  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_path = os.getenv('HOME') + '/saturi_lab_multi_nmt_low_resource'\n",
    "data_path = main_path + '/data/processed/translated_train_data.csv'\n",
    "df = pd.read_csv(data_path,)\n",
    "del df['Unnamed: 0']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer model files path\n",
    "src_tok_path = main_path + '/saved_models/tokenizer/old/spm_enc_spm16000.model'\n",
    "tgt_tok_path = main_path + '/saved_models/tokenizer/old/spm_dec_spm16000.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source tokenizer vocab size : 16009\n",
      "['▁Here', '▁is', '▁an', '▁example', '▁of', '▁source', '▁token', 'ization', '.']\n",
      "target tokenizer vocab size : 16009\n",
      "['▁이것', '은', '▁토', '큰', '화', '▁예', '시', '입니다', '.']\n"
     ]
    }
   ],
   "source": [
    "# Load tokenzier models\n",
    "\n",
    "src_tokenizer = spm.SentencePieceProcessor()#.Load(src_tok_path)\n",
    "src_tokenizer.Load(src_tok_path)\n",
    "tgt_tokenizer = spm.SentencePieceProcessor()#.Load(tgt_tok_path)\n",
    "tgt_tokenizer.Load(tgt_tok_path)\n",
    "\n",
    "print('source tokenizer vocab size :',src_tokenizer.vocab_size())\n",
    "print(src_tokenizer.EncodeAsPieces('Here is an example of source tokenization.'))\n",
    "print('target tokenizer vocab size :',tgt_tokenizer.vocab_size())\n",
    "print(tgt_tokenizer.EncodeAsPieces('이것은 토큰화 예시입니다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2,stratify=df['reg'],random_state=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cretate dataset for training; CustomDataSetforTranslation class created in src folder\n",
    "train_dataset = CustomDatasetforTranslation(train['eng'].to_numpy(),train['dial'].to_numpy(),train['reg'].to_numpy(), 128, src_tokenizer,tgt_tokenizer, True)\n",
    "valid_dataset = CustomDatasetforTranslation(test['eng'].to_numpy(), test['dial'].to_numpy(), test['reg'].to_numpy(), 128, src_tokenizer,tgt_tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Scheduler,optimizer, and loss function\n",
    "\n",
    "learningrate = utils.LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learningrate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = criterion(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train function\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 최종적으로 optimizer.apply_gradients()가 사용됩니다. \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the model\n",
    "@tf.function\n",
    "def model_validate(src, tgt, model):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "    v_loss = loss_function(gold, predictions[:, :-1])\n",
    "    \n",
    "    return v_loss, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train function\n",
    "def train(transformer,train_dataset, valid_dataset,optimizer,EPOCHS):\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        t = tqdm(train_dataset)\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, pairs in enumerate(t):\n",
    "            src, tgt = pairs\n",
    "            max_len = len(max(src,key=len))\n",
    "            enc_train = tf.keras.preprocessing.sequence.pad_sequences(src, padding='post', maxlen=max_len)\n",
    "            dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt, padding='post', maxlen=max_len)\n",
    "\n",
    "            batch_loss = train_step(enc_train,\n",
    "                                    dec_train,\n",
    "                                    transformer,\n",
    "                                    optimizer)\n",
    "\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            t.set_description_str('Bucket %2d' % (bucket))\n",
    "            t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "            \n",
    "            \n",
    "        #validation\n",
    "        total_loss_val = 0\n",
    "        tv = tqdm(valid_dataset)\n",
    "        \n",
    "        for k, vpairs in enumerate(tv) :\n",
    "            src, tgt = vpairs\n",
    "            max_len = len(max(src,key=len))\n",
    "            enc_val = tf.keras.preprocessing.sequence.pad_sequences(src, padding='post', maxlen=max_len)\n",
    "            dec_val = tf.keras.preprocessing.sequence.pad_sequences(tgt, padding='post', maxlen=max_len)\n",
    "            val_loss = model_validate(enc_val,\n",
    "                                      dec_val,\n",
    "                                      transformer)\n",
    "            total_loss_val += val_loss\n",
    "            tv.set_postfix_str('val_Loss %.4f' % (total_loss_val.numpy() / (batch_val + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model config\n",
    "import json\n",
    "config_path = main_path + '/src/utils/config.json'\n",
    "with open(config_path,'r') as f :\n",
    "    config = json.load(f)['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['src_vocab_size'] = src_tokenizer.vocab_size()\n",
    "config['tgt_vocab_size'] = tgt_tokenizer.vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vanilla_transformer import Transformer\n",
    "# model init\n",
    "transformer = Transformer(\n",
    "    n_layers=config['n_layers'],\n",
    "    d_model=config['d_model'],\n",
    "    n_heads=config['n_heads'],\n",
    "    d_ff=config['d_ff'],\n",
    "    src_vocab_size=config['src_vocab_size'],\n",
    "    tgt_vocab_size=config['tgt_vocab_size'],\n",
    "    pos_len=config['pos_len'],\n",
    "    dropout=config['dropout'],\n",
    "    shared=config['shared'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
